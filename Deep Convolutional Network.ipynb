{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring Color to Greyscale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import image\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "figsize(16, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126905 image handles\n"
     ]
    }
   ],
   "source": [
    "imdir = \"images/raw\"\n",
    "handles = os.listdir(imdir)\n",
    "random.shuffle(handles)\n",
    "print \"Found {l} image handles\".format(l=len(handles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks with Lasagne & Theano\n",
    "\n",
    "based on the lasagne example code at https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py.\n",
    "\n",
    "This is embarassingly copy-pasty. Will likely become more distinct as I grow to understand what I'm doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "\n",
    "fullsize = (\n",
    "    image.LoadColorAndGreyscaleImages(os.path.join(imdir, h))\n",
    "    for h in handles[:20000])\n",
    "\n",
    "downsampled = [\n",
    "    (image.DownsampledPatch(c, HEIGHT, WIDTH),\n",
    "     image.DownsampledPatch(g, HEIGHT, WIDTH))\n",
    "    for c, g in fullsize\n",
    "    if c is not None and g is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for c, g in downsampled[:3]:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(c)\n",
    "    ax2.imshow(g, cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = downsampled[:1000]\n",
    "val = downsampled[1000:2000]\n",
    "train = downsampled[2000:]\n",
    "\n",
    "X_test = np.array([g for _, g in test])\n",
    "y_test = np.array([c for c, _ in test])\n",
    "\n",
    "X_val = np.array([g for _, g in val])\n",
    "y_val = np.array([c for c, _ in val])\n",
    "\n",
    "X_train = np.array([g for _, g in train])\n",
    "y_train = np.array([c for c, _ in train])\n",
    "\n",
    "# c.T.astype(np.float64) / (c.T.sum(axis=0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProportionNormalizationLayer(lasagne.layers.Layer):\n",
    "\n",
    "    def get_output_for(self, inp, **kwargs):\n",
    "        return inp * inp.shape[1] / T.sum(inp, axis=1)\n",
    "\n",
    "    \n",
    "def ScaledSigmoid(beta):\n",
    "    def Closure(x):\n",
    "        return beta * T.nnet.sigmoid(x)\n",
    "    return Closure\n",
    "\n",
    "\n",
    "def BuildNet(input_var=None, height=HEIGHT, width=WIDTH):\n",
    "    # Inputs are greyscale images.\n",
    "    l_in = lasagne.layers.InputLayer(\n",
    "        shape=(None, height, width),\n",
    "        input_var=input_var)\n",
    "\n",
    "    # Shuffle them into 1-channel images. \n",
    "    l_inshuf = lasagne.layers.DimshuffleLayer(\n",
    "        l_in,\n",
    "        (0, 'x', 1, 2))\n",
    "    \n",
    "    # Apply several convolutional layers, padding at each step to\n",
    "    # maintain original image size. We first use a large number\n",
    "    # of kernels and ReLUs for feature discovery.\n",
    "    l_conv1 = lasagne.layers.Conv2DLayer(\n",
    "        l_inshuf,\n",
    "        num_filters=12,\n",
    "        filter_size=(5, 5),\n",
    "        pad=\"same\",\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    l_conv2 = lasagne.layers.Conv2DLayer(\n",
    "        l_conv1,\n",
    "        num_filters=5,\n",
    "        filter_size=(3, 3),\n",
    "        pad=\"same\",\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    # Last convolutional layer collapses back to 3 kernels, which\n",
    "    # should represent luminosity scaling factors for R, G, and B\n",
    "    # channels. We use a scaled sigmoid that produces outputs between\n",
    "    # 0 and 3, which is what we observe as a typical range of\n",
    "    # luminosity scaling factors.\n",
    "    l_conv3 = lasagne.layers.Conv2DLayer(\n",
    "        l_conv2,\n",
    "        num_filters=3,\n",
    "        filter_size=(3, 3),\n",
    "        pad=\"same\",\n",
    "        nonlinearity=ScaledSigmoid(3),\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    # Flip the index of the channel so that outputs are in the proper\n",
    "    # format for scipy color images.\n",
    "    l_outshuf = lasagne.layers.DimshuffleLayer(\n",
    "        l_conv3,\n",
    "        (0, 2, 3, 1))\n",
    "    #l_out = ProportionNormalizationLayer(l_conv3)\n",
    "    return l_outshuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Minibatches(inputs, targets, batch_size):\n",
    "    assert len(inputs) == len(targets)\n",
    "    for i in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        yield (i, inputs[i:i + batch_size], targets[i:i + batch_size])\n",
    "\n",
    "\n",
    "def CreateTheanoExprs(height=HEIGHT, width=WIDTH):\n",
    "    # Inputs and outputs.\n",
    "    input_var = T.tensor3(\"inputs\")\n",
    "    \n",
    "    # Our target_var contains raw target images, but we're not actually\n",
    "    # training on raw activation values. What we're trying to discover are\n",
    "    # scaling factors that need to be applied to greyscale luminosities for\n",
    "    # each channel to reconstruct the original image.\n",
    "    target_var = T.tensor4(\"targets\")\n",
    "    target_ratios = target_var / (target_var.mean(axis=3, keepdims=True) + 1)\n",
    "    \n",
    "    # Build network.\n",
    "    net = BuildNet(\n",
    "        input_var=input_var,\n",
    "        height=height,\n",
    "        width=width)\n",
    "\n",
    "    # Loss expression.\n",
    "    # Since we don't have stochastic dropout, we can use the same loss\n",
    "    # expr for training and validation. If we want to add a dropout layer,\n",
    "    # then we need a separate loss expression for validation where stochastic\n",
    "    # elements are explicitly frozen & dropout is disabled.\n",
    "    prediction = lasagne.layers.get_output(net)\n",
    "    loss = lasagne.objectives.squared_error(prediction, target_ratios).mean()\n",
    "\n",
    "    # Weight updates during training.\n",
    "    params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "        loss,\n",
    "        params,\n",
    "        learning_rate=0.001,\n",
    "        momentum=0.9)\n",
    "\n",
    "    # Theano function to train a mini-batch.\n",
    "    train_fn = theano.function(\n",
    "        [input_var, target_var],\n",
    "        loss,\n",
    "        updates=updates,\n",
    "        name=\"Train\")\n",
    "\n",
    "    # Thano function to evaluate / validate on an input.\n",
    "    # The difference between this and the training function is that the\n",
    "    # test / validation function does not apply weight updates.\n",
    "    val_fn = theano.function(\n",
    "        [input_var, target_var],\n",
    "        [prediction, loss],\n",
    "        name=\"Evaluate\")\n",
    "    \n",
    "    return net, train_fn, val_fn\n",
    "\n",
    "\n",
    "def Test(batch_size, X_test, y_test, net, val_fn):\n",
    "    mark = time.time()\n",
    "    test_errs = []\n",
    "    iterator = Minibatches(X_test, y_test, batch_size)\n",
    "    for i, inputs, targets in iterator:\n",
    "        print \"Testing batch starting at {i}...\".format(i=i)\n",
    "        _, err = val_fn(inputs, targets)\n",
    "        test_errs.append(err)\n",
    "    test_err = np.mean(test_errs)\n",
    "    test_time = time.time() - mark\n",
    "    print (\"\\nTesting completed in {test_time:.2f} seconds. \"\n",
    "           \"Test error = {t_err:.4f}.\").format(\n",
    "               test_time=test_time,\n",
    "               t_err=test_err)\n",
    "    return test_err\n",
    "\n",
    "\n",
    "def Train(num_epochs, batch_size,\n",
    "          X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "          net, train_fn, val_fn):\n",
    "    # Record stats for each epoch\n",
    "    epochs = []\n",
    "    \n",
    "    # Run through training data for each epoch\n",
    "    for epoch in xrange(num_epochs):\n",
    "        # Train\n",
    "        mark = time.time()\n",
    "        train_errs = []\n",
    "        iterator = Minibatches(X_train, y_train, batch_size)\n",
    "        for i, inputs, targets in iterator:\n",
    "            print \"Training batch starting at {i}...\".format(i=i)\n",
    "            train_errs.append(train_fn(inputs, targets))\n",
    "        train_err = np.mean(train_errs)\n",
    "        train_time = time.time() - mark\n",
    "        \n",
    "        # Validate\n",
    "        mark = time.time()\n",
    "        val_errs = []\n",
    "        iterator = Minibatches(X_val, y_val, batch_size)\n",
    "        for i, inputs, targets in iterator:\n",
    "            print \"Validating batch starting at {i}...\".format(i=i)\n",
    "            _, err = val_fn(inputs, targets)\n",
    "            val_errs.append(err)\n",
    "        val_err = np.mean(val_errs)\n",
    "        val_time = time.time() - mark\n",
    "        \n",
    "        # Record & report metrics\n",
    "        epochs.append((epoch, train_time, val_time, train_err, val_err))\n",
    "        print (\"\\nFinished Epoch {epoch}:\\n\"\n",
    "               \"- training took {train_time:.2f} minutes\\n\"\n",
    "               \"- validation took {val_time:.2f} minutes\\n\"\n",
    "               \"- training error = {t_err:.4f}\\n\"\n",
    "               \"- validation error = {v_err:.4f}\\n\").format(\n",
    "                   epoch=epoch,\n",
    "                   train_time=train_time / 60,\n",
    "                   val_time=val_time / 60,\n",
    "                   t_err=train_err,\n",
    "                   v_err=val_err)\n",
    "\n",
    "    test_err = Test(batch_size, X_test, y_test, net, val_fn)\n",
    "    return epochs, net, test_err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano_exprs = CreateTheanoExprs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs2, net, err = Train(\n",
    "    100, 500,\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    *theano_exprs)\n",
    "epochs += epochs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "eval_fn = theano_exprs[2]\n",
    "indices = range(len(X_test))\n",
    "random.shuffle(indices)\n",
    "\n",
    "for i in indices[:10]:\n",
    "    color = y_test[i]\n",
    "    grey = X_test[i]\n",
    "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(1, 4)\n",
    "    w, _ = eval_fn(\n",
    "        grey.reshape(1, 100, 100),\n",
    "        color.reshape(1, 100, 100, 3))\n",
    "    w = w.reshape(100, 100, 3)\n",
    "    restoration = np.stack(\n",
    "        [grey * w[:, :, 0],\n",
    "         grey * w[:, :, 1],\n",
    "         grey * w[:, :, 2]],\n",
    "        axis=2)\n",
    "    ax0.imshow(w)\n",
    "    ax1.imshow(grey, cmap=cm.gray)\n",
    "    ax2.imshow(color)\n",
    "    ax3.imshow(restoration)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(5):\n",
    "    y = y_test[i]\n",
    "    x = X_test[i].astype(np.float64)\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2)\n",
    "    ax0.imshow(y)\n",
    "    for j, color in enumerate([\"red\", \"green\", \"blue\"]):\n",
    "        scaled = y[:,:,j] / x\n",
    "        ax1.hist(scaled.reshape(np.product(scaled.shape)), color=color, alpha=0.5,\n",
    "                 histtype=\"stepfilled\", bins=np.array(range(300)) / 100.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
